# -*- coding: utf-8 -*-
"""taste_Transformers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_LanWqxiMTgu99UTs_wDSD-eTbV5QTq
"""

import scipy.io
import numpy as np
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# 1. Load your aggregated preprocessed fNIRS dataset
data = scipy.io.loadmat('All_Preprocessed_fNIRS.mat')

X = data['all_segments']       # shape (samples, time_steps, channels), e.g. (544, 1000, 16)
labels_raw = data['all_labels'].flatten()

# Convert from MATLAB cell / numpy array to list of strings if needed
labels = [str(l[0]) if (isinstance(l, np.ndarray) or isinstance(l, bytes)) else str(l) for l in labels_raw]

# 2. Encode labels to integers and then one-hot vectors
le = LabelEncoder()
y_int = le.fit_transform(labels)      # e.g. converts taste names to 0,1,2,...
y_cat = to_categorical(y_int)          # one-hot encoded labels

print(f"Classes: {le.classes_}")
print(f"Data shape: {X.shape}")
print(f"One-hot labels shape: {y_cat.shape}")

# 3. Split dataset into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(
    X, y_cat, test_size=0.2, random_state=42, stratify=y_int)

print(f"Training samples: {X_train.shape[0]}, Validation samples: {X_val.shape[0]}")

# 4. Define Transformer Encoder model (your provided code)

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
    x = layers.Dropout(dropout)(x)
    res = layers.Add()([x, inputs])

    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return layers.Add()([x, res])

def build_transformer_model(
    input_shape,
    head_size=64,
    num_heads=4,
    ff_dim=128,
    num_transformer_blocks=2,
    mlp_units=[128],
    dropout=0.1,
    mlp_dropout=0.1,
    num_classes=5
):
    inputs = layers.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = layers.GlobalAveragePooling1D()(x)
    for units in mlp_units:
        x = layers.Dense(units, activation="relu")(x)
        x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(num_classes, activation="softmax")(x)

    model = models.Model(inputs, outputs)
    model.compile(
        optimizer="adam",
        loss="categorical_crossentropy",
        metrics=["accuracy"]
    )
    return model

# 5. Instantiate and summarize model
model = build_transformer_model(
    input_shape=(X_train.shape[1], X_train.shape[2]),
    num_classes=y_cat.shape[1]
)
model.summary()

# 6. Train the model
history = model.fit(
    X_train, y_train,
    epochs=40,
    batch_size=32,
    validation_data=(X_val, y_val)
)

# 7. Plot training loss and accuracy metrics
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from itertools import cycle

# 1. Print training and validation accuracy recorded during training
train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
print(f"Max Training Accuracy: {max(train_accuracy):.4f}")
print(f"Max Validation Accuracy: {max(val_accuracy):.4f}")

# 2. Predict class probabilities on validation set
y_pred_probs = model.predict(X_val)

# 3. Convert predicted probabilities to predicted class indices
y_pred = np.argmax(y_pred_probs, axis=1)

# 4. Convert one-hot encoded true labels back to integer class indices
y_true = np.argmax(y_val, axis=1)

# 5. Print classification report (Precision, Recall, F1-score per class)
print("Classification Report:")
print(classification_report(y_true, y_pred, target_names=le.classes_))

# 6. Compute and print confusion matrix
cm = confusion_matrix(y_true, y_pred)
print("Confusion Matrix:")
print(cm)

# 7. Plot confusion matrix
plt.figure(figsize=(8,6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(len(le.classes_))
plt.xticks(tick_marks, le.classes_, rotation=45)
plt.yticks(tick_marks, le.classes_)

thresh = cm.max() / 2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, format(cm[i, j], 'd'),
                 horizontalalignment='center',
                 color='white' if cm[i, j] > thresh else 'black')

plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()

# 8. Plot ROC curves and compute AUC for each class (One-vs-Rest)
n_classes = y_val.shape[1]
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_val[:, i], y_pred_probs[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(10,8))
colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f'ROC curve of class {le.classes_[i]} (AUC = {roc_auc[i]:.2f})')

plt.plot([0,1], [0,1], 'k--', lw=2)  # diagonal line representing random guess
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

model.save('fNIRS_transformer_model.h5')